import numpy as np
import lightgbm as lgb
import catboost as cb
from sklearn.metrics import mean_squared_error, r2_score
import pickle
import os
import json
import matplotlib.pyplot as plt
import seaborn as sns

base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
data_dir = os.path.join(base_path, 'data')
models_dir = os.path.join(base_path, 'models')
evaluation_dir = os.path.join(base_path, 'evaluation_data')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
# åŠ è½½é¢„å¤„ç†åçš„æ•°æ®
def load_preprocessed_data():
    """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
    print("æ­£åœ¨åŠ è½½é¢„å¤„ç†åçš„æ•°æ®...")
    
    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))
    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))
    
    # åŠ è½½ç‰¹å¾åˆ—
    with open(os.path.join(models_dir, 'feature_columns.pkl'), 'rb') as f:
        feature_columns = pickle.load(f)
    
    print(f"æµ‹è¯•é›†: {X_test.shape}")
    
    return X_test, y_test, feature_columns

# åŠ è½½æ¨¡å‹
def load_models():
    """åŠ è½½LightGBMå’ŒCatBoostæ¨¡å‹"""
    print("\næ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½LightGBMæ¨¡å‹
    lgb_model_path = os.path.join(models_dir, 'lightgbm_model.txt')
    lgb_model = lgb.Booster(model_file=lgb_model_path)
    print(f"LightGBMæ¨¡å‹åŠ è½½æˆåŠŸ: {lgb_model_path}")
    
    # åŠ è½½CatBoostæ¨¡å‹
    cb_model_path = os.path.join(models_dir, 'catboost_model.cbm')
    cb_model = cb.CatBoostRegressor()
    cb_model.load_model(cb_model_path)
    print(f"CatBoostæ¨¡å‹åŠ è½½æˆåŠŸ: {cb_model_path}")
    
    return lgb_model, cb_model

# åŠ è½½è®­ç»ƒä¿¡æ¯
def load_training_info():
    """åŠ è½½ä¸¤ä¸ªæ¨¡å‹çš„è®­ç»ƒä¿¡æ¯"""
    print("\næ­£åœ¨åŠ è½½è®­ç»ƒä¿¡æ¯...")
    
    # åŠ è½½LightGBMè®­ç»ƒä¿¡æ¯
    with open(os.path.join(models_dir, 'training_info.pkl'), 'rb') as f:
        lgb_info = pickle.load(f)
    
    # åŠ è½½CatBoostè®­ç»ƒä¿¡æ¯
    with open(os.path.join(models_dir, 'catboost_training_info.pkl'), 'rb') as f:
        cb_info = pickle.load(f)
    
    return lgb_info, cb_info

# åŠ è½½ç‰¹å¾é‡è¦æ€§
def load_feature_importance():
    """åŠ è½½ä¸¤ä¸ªæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§"""
    print("\næ­£åœ¨åŠ è½½ç‰¹å¾é‡è¦æ€§...")
    
    # åŠ è½½LightGBMç‰¹å¾é‡è¦æ€§
    with open(os.path.join(models_dir, 'feature_importance.pkl'), 'rb') as f:
        lgb_importance = pickle.load(f)
    
    # åŠ è½½CatBoostç‰¹å¾é‡è¦æ€§
    with open(os.path.join(models_dir, 'catboost_feature_importance.pkl'), 'rb') as f:
        cb_importance = pickle.load(f)
    
    return lgb_importance, cb_importance

# æ¨¡å‹æ€§èƒ½å¯¹æ¯”
def compare_model_performance(lgb_model, cb_model, X_test, y_test):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    print("\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===")
    
    # LightGBMé¢„æµ‹
    lgb_pred = lgb_model.predict(X_test)
    lgb_mse = mean_squared_error(y_test, lgb_pred)
    lgb_rmse = np.sqrt(lgb_mse)
    lgb_r2 = r2_score(y_test, lgb_pred)
    
    # CatBoosté¢„æµ‹
    cb_pred = cb_model.predict(X_test)
    cb_mse = mean_squared_error(y_test, cb_pred)
    cb_rmse = np.sqrt(cb_mse)
    cb_r2 = r2_score(y_test, cb_pred)
    
    print("\nLightGBM æµ‹è¯•é›†æ€§èƒ½:")
    print(f"  MSE: {lgb_mse:.6f}")
    print(f"  RMSE: {lgb_rmse:.6f}")
    print(f"  RÂ²: {lgb_r2:.6f}")
    
    print("\nCatBoost æµ‹è¯•é›†æ€§èƒ½:")
    print(f"  MSE: {cb_mse:.6f}")
    print(f"  RMSE: {cb_rmse:.6f}")
    print(f"  RÂ²: {cb_r2:.6f}")
    
    # è®¡ç®—æ€§èƒ½å·®å¼‚
    print("\næ€§èƒ½å·®å¼‚:")
    print(f"  RMSEå·®å¼‚: {abs(lgb_rmse - cb_rmse):.6f} ({'CatBoostæ›´ä¼˜' if cb_rmse < lgb_rmse else 'LightGBMæ›´ä¼˜'})")
    print(f"  RÂ²å·®å¼‚: {abs(lgb_r2 - cb_r2):.6f} ({'CatBoostæ›´ä¼˜' if cb_r2 > lgb_r2 else 'LightGBMæ›´ä¼˜'})")
    
    return {
        'lightgbm': {'mse': lgb_mse, 'rmse': lgb_rmse, 'r2': lgb_r2, 'predictions': lgb_pred},
        'catboost': {'mse': cb_mse, 'rmse': cb_rmse, 'r2': cb_r2, 'predictions': cb_pred}
    }

# ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
def compare_feature_importance(lgb_importance, cb_importance, feature_columns):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§"""
    print("\n=== ç‰¹å¾é‡è¦æ€§å¯¹æ¯” ===")
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    lgb_imp_dict = dict(lgb_importance)
    cb_imp_dict = dict(cb_importance)
    
    # å½’ä¸€åŒ–ç‰¹å¾é‡è¦æ€§
    lgb_total = sum(lgb_imp_dict.values())
    cb_total = sum(cb_imp_dict.values())
    
    lgb_imp_norm = {k: v/lgb_total for k, v in lgb_imp_dict.items()}
    cb_imp_norm = {k: v/cb_total for k, v in cb_imp_dict.items()}
    
    # æ‰“å°Top 10ç‰¹å¾å¯¹æ¯”
    print("\nTop 10 ç‰¹å¾é‡è¦æ€§å¯¹æ¯”:")
    print(f"{'ç‰¹å¾':<35} {'LightGBM':<15} {'CatBoost':<15}")
    print("-" * 65)
    
    all_features = sorted(set(lgb_imp_dict.keys()) | set(cb_imp_dict.keys()), 
                         key=lambda x: max(lgb_imp_norm.get(x, 0), cb_imp_norm.get(x, 0)), 
                         reverse=True)
    
    for feature in all_features[:10]:
        lgb_val = lgb_imp_norm.get(feature, 0) * 100
        cb_val = cb_imp_norm.get(feature, 0) * 100
        print(f"{feature:<35} {lgb_val:>6.2f}%        {cb_val:>6.2f}%")
    
    return lgb_imp_norm, cb_imp_norm

# ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾
def generate_comparison_plots(performance, lgb_imp_norm, cb_imp_norm, y_test):
    """ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
    print("\n=== ç”Ÿæˆå¯¹æ¯”å›¾è¡¨ ===")
    
    # 1. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    metrics = ['MSE', 'RMSE', 'RÂ²']
    lgb_values = [performance['lightgbm']['mse'], performance['lightgbm']['rmse'], performance['lightgbm']['r2']]
    cb_values = [performance['catboost']['mse'], performance['catboost']['rmse'], performance['catboost']['r2']]
    
    for i, (metric, lgb_val, cb_val) in enumerate(zip(metrics, lgb_values, cb_values)):
        axes[i].bar(['LightGBM', 'CatBoost'], [lgb_val, cb_val], color=['#3498db', '#e74c3c'])
        axes[i].set_title(f'{metric} å¯¹æ¯”')
        axes[i].set_ylabel(metric)
        
        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for j, val in enumerate([lgb_val, cb_val]):
            axes[i].text(j, val, f'{val:.4f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(os.path.join(evaluation_dir, 'model_performance_comparison.png'), dpi=300, bbox_inches='tight')
    print("æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: model_performance_comparison.png")
    plt.close()
    
    # 2. ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆTop 10ï¼‰
    top_features = sorted(lgb_imp_norm.keys(), 
                         key=lambda x: max(lgb_imp_norm[x], cb_imp_norm.get(x, 0)), 
                         reverse=True)[:10]
    
    lgb_top_values = [lgb_imp_norm[f] * 100 for f in top_features]
    cb_top_values = [cb_imp_norm.get(f, 0) * 100 for f in top_features]
    
    x = np.arange(len(top_features))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.barh(x - width/2, lgb_top_values, width, label='LightGBM', color='#3498db')
    ax.barh(x + width/2, cb_top_values, width, label='CatBoost', color='#e74c3c')
    
    ax.set_ylabel('ç‰¹å¾')
    ax.set_xlabel('é‡è¦æ€§ (%)')
    ax.set_title('Top 10 ç‰¹å¾é‡è¦æ€§å¯¹æ¯”')
    ax.set_yticks(x)
    ax.set_yticklabels(top_features)
    ax.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(evaluation_dir, 'feature_importance_comparison.png'), dpi=300, bbox_inches='tight')
    print("ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾å·²ä¿å­˜: feature_importance_comparison.png")
    plt.close()
    
    # 3. é¢„æµ‹å€¼å¯¹æ¯”æ•£ç‚¹å›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # LightGBM
    axes[0].scatter(y_test, performance['lightgbm']['predictions'], alpha=0.5, s=10)
    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[0].set_xlabel('çœŸå®å€¼')
    axes[0].set_ylabel('é¢„æµ‹å€¼')
    axes[0].set_title(f'LightGBM é¢„æµ‹ vs çœŸå®å€¼ (RÂ²={performance["lightgbm"]["r2"]:.4f})')
    axes[0].grid(True, alpha=0.3)
    
    # CatBoost
    axes[1].scatter(y_test, performance['catboost']['predictions'], alpha=0.5, s=10, color='#e74c3c')
    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[1].set_xlabel('çœŸå®å€¼')
    axes[1].set_ylabel('é¢„æµ‹å€¼')
    axes[1].set_title(f'CatBoost é¢„æµ‹ vs çœŸå®å€¼ (RÂ²={performance["catboost"]["r2"]:.4f})')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(evaluation_dir, 'predictions_comparison.png'), dpi=300, bbox_inches='tight')
    print("é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜: predictions_comparison.png")
    plt.close()

# ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
def save_comparison_report(performance, lgb_info, cb_info, lgb_imp_norm, cb_imp_norm):
    """ä¿å­˜å¯¹æ¯”æŠ¥å‘Šä¸ºJSON"""
    print("\n=== ä¿å­˜å¯¹æ¯”æŠ¥å‘Š ===")
    
    report = {
        'performance': {
            'lightgbm': {
                'mse': float(performance['lightgbm']['mse']),
                'rmse': float(performance['lightgbm']['rmse']),
                'r2': float(performance['lightgbm']['r2'])
            },
            'catboost': {
                'mse': float(performance['catboost']['mse']),
                'rmse': float(performance['catboost']['rmse']),
                'r2': float(performance['catboost']['r2'])
            }
        },
        'training_info': {
            'lightgbm': {
                'training_time': lgb_info['training_time'],
                'best_iteration': lgb_info['best_iteration']
            },
            'catboost': {
                'training_time': cb_info['training_time'],
                'best_iteration': cb_info['best_iteration']
            }
        },
        'feature_importance': {
            'lightgbm': {k: float(v) for k, v in lgb_imp_norm.items()},
            'catboost': {k: float(v) for k, v in cb_imp_norm.items()}
        },
        'winner': 'CatBoost' if performance['catboost']['r2'] > performance['lightgbm']['r2'] else 'LightGBM',
        'r2_improvement': abs(performance['catboost']['r2'] - performance['lightgbm']['r2'])
    }
    
    report_path = os.path.join(evaluation_dir, 'model_comparison_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("æ¨¡å‹å¯¹æ¯”æ€»ç»“".center(60))
    print("="*60)
    print(f"\nğŸ† ç»¼åˆè¡¨ç°æœ€ä¼˜: {report['winner']}")
    print(f"\nğŸ“Š RÂ² æå‡å¹…åº¦: {report['r2_improvement']:.6f}")
    print(f"\nâ±ï¸  è®­ç»ƒæ—¶é—´å¯¹æ¯”:")
    print(f"   LightGBM: {lgb_info['training_time']:.2f}ç§’")
    print(f"   CatBoost: {cb_info['training_time']:.2f}ç§’")
    print("\n" + "="*60)

if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    X_test, y_test, feature_columns = load_preprocessed_data()
    
    # åŠ è½½æ¨¡å‹
    lgb_model, cb_model = load_models()
    
    # åŠ è½½è®­ç»ƒä¿¡æ¯
    lgb_info, cb_info = load_training_info()
    
    # åŠ è½½ç‰¹å¾é‡è¦æ€§
    lgb_importance, cb_importance = load_feature_importance()
    
    # æ€§èƒ½å¯¹æ¯”
    performance = compare_model_performance(lgb_model, cb_model, X_test, y_test)
    
    # ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
    lgb_imp_norm, cb_imp_norm = compare_feature_importance(lgb_importance, cb_importance, feature_columns)
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    generate_comparison_plots(performance, lgb_imp_norm, cb_imp_norm, y_test)
    
    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    save_comparison_report(performance, lgb_info, cb_info, lgb_imp_norm, cb_imp_norm)
    
    print("\n=== æ¨¡å‹å¯¹æ¯”åˆ†æå®Œæˆ ===")
